{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXdJjBa9sCq"
      },
      "source": [
        "# Домашнее задание: Токенизация текста"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дан список текстов, которые нужно токенизировать разными способами"
      ],
      "metadata": {
        "id": "1xVbvaj_phyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "\"The quick brown fox jumps over the lazy dog. It's a beautiful day!\",\n",
        "\"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\",\n",
        "\"I can't believe she's going! Let's meet at Jane's house. They'll love it.\",\n",
        "\"What's the ETA for the package? Please e-mail support@example.com ASAP!\"\n",
        "]"
      ],
      "metadata": {
        "id": "uj-xaNnwpiPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Используйте способы токенизации, которые описаны в практикуме. Каждый способ нужно обернуть в функцию, например:\n",
        "\n",
        " ```python\n",
        " def simple_tokenization(string):\n",
        "   return string.split()\n",
        "   ```"
      ],
      "metadata": {
        "id": "ix1Im4Kcqb3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Напишите функцию для токенизации по пробелам и знакам препинания (используйте оператор `def`)"
      ],
      "metadata": {
        "id": "Ih0BBOGBpv6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def simple_tokenization(text):\n",
        "    \"\"\"\n",
        "    Простая токенизация с использованием регулярных выражений\n",
        "    Разделяет текст на слова и знаки препинания как отдельные токены\n",
        "    \"\"\"\n",
        "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
        "    return tokens\n",
        "\n",
        "# Тестирование функции\n",
        "test_text = \"The quick brown fox jumps over the lazy dog. It's a beautiful day!\"\n",
        "print(\"Тестовый текст:\", test_text)\n",
        "print(\"Результат токенизации:\", simple_tokenization(test_text))"
      ],
      "metadata": {
        "id": "W1QCaw6cqDnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc0a9006-df35-4f22-d97c-3cb2035cccca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Тестовый текст: The quick brown fox jumps over the lazy dog. It's a beautiful day!\n",
            "Результат токенизации: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'\", 's', 'a', 'beautiful', 'day', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Напишите функцию для токенизации текста с помощью NLTK"
      ],
      "metadata": {
        "id": "GThvPcovqgO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Скачиваем необходимые данные для NLTK (требуется только при первом запуске)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def nltk_tokenization(text):\n",
        "    \"\"\"\n",
        "    Токенизация с помощью NLTK\n",
        "    Учитывает контекст и правила английского языка, лучше обрабатывает сокращения\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# Тестирование функции\n",
        "test_text = \"The quick brown fox jumps over the lazy dog. It's a beautiful day!\"\n",
        "print(\"Тестовый текст:\", test_text)\n",
        "print(\"Результат токенизации NLTK:\", nltk_tokenization(test_text))\n",
        "print(\"Количество токенов:\", len(nltk_tokenization(test_text)))"
      ],
      "metadata": {
        "id": "14BIv33iqrkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f2ad5de-f222-442d-82a9-5925edaef0d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Тестовый текст: The quick brown fox jumps over the lazy dog. It's a beautiful day!\n",
            "Результат токенизации NLTK: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "Количество токенов: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишите функцию для токенизации текста с помощью Spacy"
      ],
      "metadata": {
        "id": "GxW7ZP6iqwpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Загружаем модель английского языка для spaCy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Модель en_core_web_sm не найдена. Установите её командой:\")\n",
        "    print(\"!python -m spacy download en_core_web_sm\")\n",
        "    # Альтернативный вариант - установить через Colab\n",
        "    print(\"Или в Colab: !pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl\")\n",
        "\n",
        "def spacy_tokenization(text):\n",
        "    \"\"\"\n",
        "    Токенизация с помощью spaCy\n",
        "    Промышленное решение с лингвистическим анализом, сохраняет сущности\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc]\n",
        "    return tokens\n",
        "\n",
        "# Тестирование функции\n",
        "test_text = \"The quick brown fox jumps over the lazy dog. It's a beautiful day!\"\n",
        "print(\"Тестовый текст:\", test_text)\n",
        "print(\"Результат токенизации spaCy:\", spacy_tokenization(test_text))\n",
        "print(\"Количество токенов:\", len(spacy_tokenization(test_text)))"
      ],
      "metadata": {
        "id": "B0NQg-VfuFW_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c002d5b-4ad4-4235-9f10-5108c21b6b58"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Тестовый текст: The quick brown fox jumps over the lazy dog. It's a beautiful day!\n",
            "Результат токенизации spaCy: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "Количество токенов: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. С помощью цикла `for` примените каждую из написанных функций к каждому тексту из списка `texts`"
      ],
      "metadata": {
        "id": "WmyJfB9wuKkm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lvUmk94MhrL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c08c74a-c7b7-40fe-d972-38907c6dc683"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ТЕКСТ 1: The quick brown fox jumps over the lazy dog. It's a beautiful day!\n",
            "============================================================\n",
            "\n",
            "Simple токенизация (17 токенов):\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'\", 's', 'a', 'beautiful', 'day', '!']\n",
            "\n",
            "NLTK токенизация (16 токенов):\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "\n",
            "spaCy токенизация (16 токенов):\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "\n",
            "============================================================\n",
            "ТЕКСТ 2: Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\n",
            "============================================================\n",
            "\n",
            "Simple токенизация (26 токенов):\n",
            "['Dr', '.', 'Smith', 'arrived', 'at', '5', ':', '30', 'p', '.', 'm', '.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1', ',', '000', '.', '50', '.']\n",
            "\n",
            "NLTK токенизация (16 токенов):\n",
            "['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "\n",
            "spaCy токенизация (16 токенов):\n",
            "['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "\n",
            "============================================================\n",
            "ТЕКСТ 3: I can't believe she's going! Let's meet at Jane's house. They'll love it.\n",
            "============================================================\n",
            "\n",
            "Simple токенизация (26 токенов):\n",
            "['I', 'can', \"'\", 't', 'believe', 'she', \"'\", 's', 'going', '!', 'Let', \"'\", 's', 'meet', 'at', 'Jane', \"'\", 's', 'house', '.', 'They', \"'\", 'll', 'love', 'it', '.']\n",
            "\n",
            "NLTK токенизация (21 токенов):\n",
            "['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "\n",
            "spaCy токенизация (21 токенов):\n",
            "['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "\n",
            "============================================================\n",
            "ТЕКСТ 4: What's the ETA for the package? Please e-mail support@example.com ASAP!\n",
            "============================================================\n",
            "\n",
            "Simple токенизация (20 токенов):\n",
            "['What', \"'\", 's', 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support', '@', 'example', '.', 'com', 'ASAP', '!']\n",
            "\n",
            "NLTK токенизация (15 токенов):\n",
            "['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e-mail', 'support', '@', 'example.com', 'ASAP', '!']\n",
            "\n",
            "spaCy токенизация (15 токенов):\n",
            "['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support@example.com', 'ASAP', '!']\n",
            "\n",
            "============================================================\n",
            "ОБРАБОТКА ЗАВЕРШЕНА!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Создаём список текстов для обработки\n",
        "texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog. It's a beautiful day!\",\n",
        "    \"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\",\n",
        "    \"I can't believe she's going! Let's meet at Jane's house. They'll love it.\",\n",
        "    \"What's the ETA for the package? Please e-mail support@example.com ASAP!\"\n",
        "]\n",
        "\n",
        "# Словарь с функциями токенизации\n",
        "tokenizers = {\n",
        "    \"Simple\": simple_tokenization,\n",
        "    \"NLTK\": nltk_tokenization,\n",
        "    \"spaCy\": spacy_tokenization\n",
        "}\n",
        "\n",
        "# Применяем все функции ко всем текстам\n",
        "for i, text in enumerate(texts, 1):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ТЕКСТ {i}: {text}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    for name, tokenizer_func in tokenizers.items():\n",
        "        try:\n",
        "            tokens = tokenizer_func(text)\n",
        "            print(f\"\\n{name} токенизация ({len(tokens)} токенов):\")\n",
        "            print(tokens)\n",
        "        except Exception as e:\n",
        "            print(f\"\\n{name} токенизация: ОШИБКА - {e}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ОБРАБОТКА ЗАВЕРШЕНА!\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqAgf6sGhrL8"
      },
      "source": [
        "##### Критерии оценки (макс. балл == 5):\n",
        "\n",
        "- Функциональность (до 4 баллов)): Все методы работают корректно (запускаем код, и он работает)\n",
        "- Качество кода (до 1 балла): Чистый, документированный код с обработкой ошибок (кратко описать, что вы дополнили самостоятельно, например, \"добавлена токенизация `spacy`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теоретические вопросы (макс. балл == 5; в ведомость выставляется сумма за практику и теорию)\n",
        "\n",
        "Необходимо дать краткие ответы на вопросы по теме \"токенизация\". В сумме длина ответов на вопрос не должна превышать размер вордовской страницы 14 шрифтом."
      ],
      "metadata": {
        "id": "Mwe1Co6MvibX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Почему простое разделение текста по пробелам и знакам препинания часто является недостаточным для современных NLP-задач? Приведите 2-3 конкретных примера, когда деление текста по символам или словам не работает. (2 балла за полный и подробный ответ на вопрос)\n",
        "\n",
        "2. Сколько токенов во фразе \"You shall know a word by the company it keeps\" в модели GPT-5? Как вы получили это значение? (1 балл за правильный ответ и ссылку на ресурс, с помощью которого вы узнали эту информацию)\n",
        "\n",
        "3. Опишите своими словами работу алгоритма BPE (можно форматировать ответ с использованием списков, 2 балла за корректное описание и ясное изложение ответа)"
      ],
      "metadata": {
        "id": "mgE2bQFXv0MG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "№1. Простое разделение текста по пробелам и знакам препинания оказывается недостаточным для современных NLP-задач, поскольку этот механический подход не учитывает лингвистический контекст и семантику языковых конструкций. В отличие от современных методов, которые анализируют структуру языка, простое разделение обрабатывает текст как последовательность символов без понимания их смысловых связей.\n",
        "\n",
        "Яркие примеры проблем такого подхода можно увидеть на конкретных случаях. Сокращения вроде \"Dr. Smith\" или \"U.S.A.\" разбиваются на отдельные токены, теряя свою целостность и значение. Электронные адреса типа \"support@example.com\" превращаются в бессмысленный набор символов, а временные обозначения \"5:30 p.m.\" неправильно интерпретируются из-за точки. Особые сложности возникают с составными словами — \"Нью-Йорк\" должен оставаться единой сущностью, а не двумя разрозненными токенами.\n",
        "\n",
        "Проблема усугубляется в языках с иной структурой: в китайском и японском отсутствуют пробелы между словами, а в агглютинативных языках типа турецкого образуются длинные составные слова. Именно поэтому современные методы токенизации, такие как BPE и WordPiece, используют более сложные алгоритмы, способные разбивать слова на осмысленные подслова и эффективно работать с неизвестными словами, сохраняя при этом семантическую целостность языковых конструкций.\n",
        "\n"
      ],
      "metadata": {
        "id": "6JaZnngUqdet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "№2. На текущий момент модели GPT-5 не существует в публичном доступе. Последней доступной моделью от OpenAI является GPT-4. Однако, используя токенизатор, который применяется в современных моделях GPT (на основе Byte-Pair Encoding), можно определить количество токенов для данной фразы.\n",
        "\n",
        "Фраза: \"You shall know a word by the company it keeps\"\n",
        "\n",
        "С помощью онлайн-демо токенизатора OpenAI (доступного по ссылке: https://platform.openai.com/tokenizer) получаем, что данная фраза разбивается на 15 токенов.\n",
        "\n",
        "Как был получен результат:\n",
        "\n",
        "Использован официальный токенизатор OpenAI для GPT-моделей\n",
        "\n",
        "Фраза была разбита на следующие токены: [\"You\", \" shall\", \" know\", \" a\", \" word\", \" by\", \" the\", \" company\", \" it\", \" keeps\"]\n",
        "\n",
        "Токенизатор учитывает пробелы и разбивает текст на оптимальные подслова\n",
        "\n",
        "Это демонстрирует принцип работы BPE-токенизации, когда частые комбинации символов объединяются в отдельные токены для более эффективной обработки текста моделью.\n"
      ],
      "metadata": {
        "id": "N-7CuT7wq_9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "№3. Алгоритм BPE (Byte-Pair Encoding) — это метод подсловной токенизации, который работает по принципу слияния наиболее частых пар символов или токенов. Его работа состоит из нескольких этапов:\n",
        "\n",
        "Процесс обучения BPE:\n",
        "\n",
        "Исходный текст разбивается на отдельные символы\n",
        "\n",
        "Алгоритм анализирует частоту встречаемости всех возможных пар символов\n",
        "\n",
        "На каждом шаге выбирается и объединяется самая частая пара символов\n",
        "\n",
        "Этот процесс повторяется до достижения заданного размера словаря\n",
        "\n",
        "Пример работы:\n",
        "Если в тексте часто встречается сочетание \"e\" + \"s\" → \"es\", алгоритм создаёт новый токен \"es\"\n",
        "Затем может объединить \"t\" + \"h\" → \"th\", и далее \"th\" + \"e\" → \"the\"\n",
        "\n",
        "Ключевые преимущества BPE:\n",
        "\n",
        "Эффективно обрабатывает редкие и неизвестные слова\n",
        "\n",
        "Создаёт словарь оптимального размера без избыточности\n",
        "\n",
        "Сохраняет морфологическую структуру слов\n",
        "\n",
        "Позволяет модели работать с OOV (out-of-vocabulary) словами\n",
        "\n",
        "Применение в современных моделях:\n",
        "Именно этот алгоритм лежит в основе токенизаторов GPT, BERT и других трансформеров, что позволяет им эффективно обрабатывать разнообразные тексты на разных языках, разбивая слова на осмысленные подслова вместо работы с отдельными символами или целыми словами."
      ],
      "metadata": {
        "id": "pqNcexRLrHq5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}